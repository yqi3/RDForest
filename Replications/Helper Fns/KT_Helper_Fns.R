######################################
# Helper functions for Keele & Titiunik (2015)
######################################
set.seed(1234)
library(dplyr)
library(purrr)
library(DiceKriging)

#### Coin flip function according to predicted probability ####
bern <- function(prob) {
  return(as.numeric(rbernoulli(n=1,p=prob)))
}

#### Function that ensures bounded noise generated by rnorm() in genDGP ####
bounded_noise <- function(noise) {
  return(max(min(noise,10),-10))
}

### Generate simulation data ###
#### genDGP is a function that generates simulation data given sample size n and desired outcome variable ####
genDGP <- function(n, outcome){
  ### trt reference points for "ignored" data points, which simplifies the classification process ###
  trt_ref1_x <- pointsALL$longitude[26]
  trt_ref1_y <- pointsALL$latitude[26]
  trt_ref2_x <- pointsALL$longitude[80]
  trt_ref2_y <- pointsALL$latitude[80]
  trt_slope <- (trt_ref2_y-trt_ref1_y)/(trt_ref2_x-trt_ref1_x)
  trt_intercept <- trt_ref1_y-trt_slope*trt_ref1_x
  
  ### ctrl reference points for "ignored" data points, which simplifies the classification process ###
  ctrl_ref1_x <- pointsALL$longitude[1]
  ctrl_ref1_y <- pointsALL$latitude[1]
  ctrl_ref2_x <- pointsALL$longitude[66]
  ctrl_ref2_y <- pointsALL$latitude[66]
  ctrl_slope <- (ctrl_ref2_y-ctrl_ref1_y)/(ctrl_ref2_x-ctrl_ref1_x)
  ctrl_intercept <- ctrl_ref1_y-ctrl_slope*ctrl_ref1_x
  
  # Restrict range
  rand <- sample_n(data[data$latitude>=min(pointsALL$latitude) & data$latitude<=max(pointsALL$latitude) & data$longitude>=min(pointsALL$longitude) & data$longitude<=max(pointsALL$longitude), ], n, replace=TRUE)
  latitude <- rand$latitude+sapply(rnorm(n, 0, sd=0.01), FUN=bounded_noise)
  longitude <- rand$longitude+sapply(rnorm(n, 0, sd=0.01), FUN=bounded_noise)
  
  simdata <- as.data.frame(cbind("latitude"=latitude, "longitude"=longitude))
  simdata$treat <- NA
  
  simdata$treat[(simdata$latitude < simdata$longitude*trt_slope+trt_intercept) | (simdata$latitude<pointsALL$latitude[45] & simdata$longitude<pointsALL$longitude[45])] <- 1
  simdata$treat[(simdata$latitude > simdata$longitude*ctrl_slope+ctrl_intercept) | (simdata$latitude>pointsALL$latitude[44] & simdata$longitude>pointsALL$longitude[44])] <- 0
  
  for (i in 1:n) {# Identify "region" based on longitude
    if ((simdata$latitude[i] < simdata$longitude[i]*trt_slope+trt_intercept) | (simdata$latitude[i]<pointsALL$latitude[45] & simdata$longitude[i]<pointsALL$longitude[45])) {
      simdata$treat[i] <- 1
    } else if ((simdata$latitude[i] > simdata$longitude[i]*ctrl_slope+ctrl_intercept) | (simdata$latitude[i]>pointsALL$latitude[44] & simdata$longitude[i]>pointsALL$longitude[44])) {
      simdata$treat[i] <- 0
    } else {
      closest_x <- which.min(abs(pointsALL[,"longitude"] - simdata$longitude[i]))
      
      # Region 1, 3, 5, 7: 1-26, 43-49, 66-73, 78-81
      # Longitude as a function of latitude
      if ((closest_x <= 26) | (closest_x >= 43 & closest_x <= 49) | 
          (closest_x >= 66 & closest_x <= 73) | (closest_x >= 78 & closest_x <= 81)) {
        # Find closest point (index) in terms of latitude in this region
        blocks <- list(c(1:26), c(43:49), c(66:73), c(78:81))
        for (j in 1:length(blocks)) {
          if (closest_x %in% blocks[[j]]) {region <- blocks[[j]]}}
        index1 <- which.min(abs(pointsALL[region,"latitude"] - simdata$latitude[i])) + region[1]-1
        if (pointsALL$latitude[index1] < simdata$latitude[i]){ 
          # simulation point is higher
          # index should decrease by 1 so latitude is bigger, fit a line between the 2 indices
          
          if (index1 == 1) { # Boundary situation, simply compare longitudes
            if (simdata$longitude[i] >= pointsALL$longitude[1]) {
              simdata$treat[i] <- 0}
            else {simdata$treat[i] <- 1}
          }
          
          else { # Not boundary, can subtract 1 to find the other index and fit a line
            index2 <- index1-1
            
            # Fitting a line (note: this is equivalent to "x as a function of y")
            slope_original <- (pointsALL$latitude[index1] - pointsALL$latitude[index2])/(pointsALL$longitude[index1] - pointsALL$longitude[index2])
            slope <- 1/slope_original
            intercept <- -(pointsALL$latitude[index1] - pointsALL$longitude[index1]*slope_original)/slope_original
            
            if (simdata$longitude[i] > simdata$latitude[i]*slope+intercept){
              # Simulation point is above the fitted line, not treated
              simdata$treat[i] <- 0
            }
            else{simdata$treat[i] <- 1}}
        }
        
        else {# simulation point is lower
          # index should increase by 1 so latitude is smaller, fit a line between the 2 indices
          index2 <- index1+1    # No boundary situation here
          
          # Fitting a line (note: this is equivalent to "x as a function of y")  
          slope_original <- (pointsALL$latitude[index1] - pointsALL$latitude[index2])/(pointsALL$longitude[index1] - pointsALL$longitude[index2])
          slope <- 1/slope_original
          intercept <- -(pointsALL$latitude[index1] - pointsALL$longitude[index1]*slope_original)/slope_original
          
          if (simdata$longitude[i] > simdata$latitude[i]*slope+intercept){
            simdata$treat[i] <- 0}
          else{simdata$treat[i] <- 1}}
      }
      
      # Region 2, 4, 6, 8: 26-43, 49-66, 73-78, 82-89
      # Latitude as a function of longitude
      if ((closest_x >= 26 & closest_x <= 43) | (closest_x >= 49 & closest_x <= 66) | 
          (closest_x >= 73 & closest_x <= 78) | (closest_x > 81 & closest_x <= 89)) {
        index1 <- closest_x
        
        if (pointsALL$longitude[index1] > simdata$longitude[i]){
          index2 <- index1-1
          slope <- (pointsALL$latitude[index1] - pointsALL$latitude[index2])/(pointsALL$longitude[index1] - pointsALL$longitude[index2])
          intercept <- pointsALL$latitude[index1] - pointsALL$longitude[index1]*slope
          
          if (simdata$latitude[i] > simdata$longitude[i]*slope+intercept) {
            simdata$treat[i] <- 0}
          else {simdata$treat[i] <- 1}}
        
        else {
          if (index1 == 89) { # Boundary situation here
            if (pointsALL$latitude[index1] < simdata$latitude[i]) {
              simdata$treat[i] <- 0}
            else {simdata$treat[i] <- 1}}
          
          else {
            index2 <- index1+1
            slope <- (pointsALL$latitude[index1] - pointsALL$latitude[index2])/(pointsALL$longitude[index1] - pointsALL$longitude[index2])
            intercept <- pointsALL$latitude[index1] - pointsALL$longitude[index1]*slope
            
            if (simdata$latitude[i] > simdata$longitude[i]*slope+intercept) {
              simdata$treat[i] <- 0}
            else {simdata$treat[i] <- 1}}
        }
      }
    }
  }
  
  X1 <- simdata$latitude
  X2 <- simdata$longitude
  
  model.df <- data.frame(X1=X1, X1.2=(X1 - mean_X1)^2, X1.3=(X1 - mean_X1)^3,
                         X2=X2, X2.2=(X2 - mean_X2)^2, X2.3=(X2 - mean_X2)^3,
                         X12=(X1 - mean_X1)*(X2 - mean_X2), 
                         W=simdata$treat)
  
  if (outcome=="turnout") {
    probs <- predict(cef, model.df, type = "response")
    simdata$e2008g <- sapply(probs, FUN=bern)
  } else if (outcome=="age") {
    simdata$age <- predict(cef, model.df)+rnorm(n,mean=0,sd=sigma_y)
  } else {
    simdata$price <- predict(cef, model.df)+rnorm(n,mean=0,sd=sigma_y)
  }
  return(simdata)
}

#### get_fit_draws is a function that generates a dataframe with num.params columns of runif values ####
### Argument: n_draws, which is the number of random combinations
get_fit_draws <- function(n_draws, tune.parameters){
  num.params <- length(tune.parameters)
  unif <- runif(n_draws * num.params)
  fit_draws <- matrix(unif, n_draws, num.params,
                      dimnames = list(NULL, tune.parameters))
  return(fit_draws)
}

#### get_draw_parameters is a function that produces a dataframe with random draws of parameter values ####
### Arguments: fit_draws generated by get_fit_draws
get_draw_parameters <- function(fit_draws, tune.parameters) {
  draw_parameters <- fit_draws
  for (i in c(1:length(tune.parameters))) {
    if (tune.parameters[i]=="alpha") {
      draw_parameters[,i] <- draw_parameters[,i] / 5  # random alphas
    } else if (tune.parameters[i]=="imbalance.penalty") {
      draw_parameters[,i] <- -log(draw_parameters[,i])  # random imbalance.penalties
    } else if (tune.parameters[i]=="honesty.fraction") {
      draw_parameters[,i] <- 0.5 + (0.8 - 0.5) * draw_parameters[,i]  # random honesty.fractions
    } else {
      draw_parameters[,i] <- ifelse(draw_parameters[,i] < 0.5, TRUE, FALSE)  # random honesty.prune.leaves
    }
  }
  
  return(draw_parameters)
}

#### get_forest_debiased_error is a function that builds a forest using a particular combination of parameter values, and outputs that forest's mean squared debiased error ####
### Arguments: a row in the dataframe generated by draw_parameters, plus the number of trees and the type of the forest ("trt" or "ctrl")
get_forest_debiased_error <- function(param) {
  a <- as.double(param[1])
  imb <- as.double(param[2])
  h <- as.double(param[3])
  prune <- as.double(param[4])
  B <- as.numeric(param[5])
  type <- param[6]
  
  d <- ncol(X_global)  # dimension, same for trt and ctrl
  lower_bound <- (1+ (1/d)*log(1-a) / log(a))^(-1)
  s <- scale_global*ceiling(nrow(X_global)^lower_bound)/nrow(X_global)
  forest <- regression_forest(X_global, Y_global, mtry=1, num.trees=B, sample.fraction=s, honesty.fraction = h, alpha = a, imbalance.penalty = imb, honesty.prune.leaves = prune)
  
  return(mean(forest$debiased.error, na.rm = TRUE))
}


#### get_kriging_model is a function that produces the Dice Kriging model for tuning ####
get_kriging_model <- function(fit_draws, small_forest_errors) {
  variance_guess <- rep(var(small_forest_errors) / 2, nrow(fit_draws))
  kriging_model <- tryCatch({
    capture.output(
      model <- km(
        design = data.frame(fit_draws),
        response = small_forest_errors,
        noise.var = variance_guess
      )
    )
    return(model)
  },
  error = function(e) {
    warning(paste0("Dicekriging threw the following error during forest tuning: \n", e))
    return(NULL)
  })
}

#### tuned_RDForest is a function that builds a random forest object from data on outcome and covariates, while tuning the parameters specified by the user
tuned_RDForest <- function(X, Y, type, num.trees=5000, tune.parameters=c("alpha", "imbalance.penalty", "honesty.fraction", "honesty.prune.leaves"), scale=0.4, tune.num.trees=50, tune.num.reps=100, tune.num.draws=1000, moderate.num.trees=tune.num.trees*4) {
  d <- ncol(X)
  lower_bound_default <- (1+ (1/d)*log(1-0.05) / log(0.05))^(-1)  # default alpha=0.05
  
  ## Precheck of tuning parameter and type names
  names_supported <- c("alpha", "imbalance.penalty", "honesty.fraction", "honesty.prune.leaves")  # we support tuning of these parameters
  if (length(setdiff(tune.parameters, names_supported)) != 0 | length(tune.parameters) != length(unique(tune.parameters))) {
    stop("Some of your specified tuning parameters are repeated or not supported by the tuned_RDForest function. For tuning none of the parameters, use \"tune.parameters=c()\".")
  }
  
  if (type != "trt" & type != "ctrl") {
    stop("Invalid type name specified. Should be either \"trt\" or \"ctrl\".")
  }
  
  if (length(tune.parameters)==0) {
    s_default <- scale*ceiling(nrow(X)^lower_bound_default)/nrow(X)
    forest_default <- regression_forest(X, Y, mtry=1, num.trees=num.trees, sample.fraction=s_default)
    return(forest_default)
  }
  
  ## Make global variables for the apply() function below
  X_global <<- X
  Y_global <<- Y
  scale_global <<- scale
  
  ## Parameter tuning
  fit.draws <- get_fit_draws(tune.num.reps, tune.parameters)
  draw.parameters <- get_draw_parameters(fit.draws, tune.parameters)
  default_names <- setdiff(names_supported, colnames(draw.parameters))  # variables not tuned, so default values need to be filled
  for (name in default_names) {
    if (name=="alpha") {
      draw.parameters <- cbind(draw.parameters, rep(0.05, tune.num.reps))
      colnames(draw.parameters)[ncol(draw.parameters)] <- "alpha"
    } else if (name=="imbalance.penalty") {
      draw.parameters <- cbind(draw.parameters, rep(0, tune.num.reps))
      colnames(draw.parameters)[ncol(draw.parameters)] <- "imbalance.penalty"
    } else if (name=="honesty.fraction") {
      draw.parameters <- cbind(draw.parameters, rep(0.5, tune.num.reps))
      colnames(draw.parameters)[ncol(draw.parameters)] <- "honesty.fraction"
    } else if (name=="honesty.prune.leaves") {
      draw.parameters <- cbind(draw.parameters, rep(TRUE, tune.num.reps))
      colnames(draw.parameters)[ncol(draw.parameters)] <- "honesty.prune.leaves"
    }
  }
  draw.parameters <- draw.parameters[,names_supported] # re-order
  draw.parameters <- cbind(draw.parameters, rep(tune.num.trees, tune.num.reps)) # also put tune.num.trees into draw.parameters for row-wise operation below
  draw.parameters <- cbind(draw.parameters, rep(type, tune.num.reps)) # also put type into draw.parameters for row-wise operation below
  
  # 1. Obtain forest errors (average debiased error across observations)
  small.forest.errors <- apply(draw.parameters, 1, get_forest_debiased_error)
  
  # 2. Fit the 'dice kriging' model to these error estimates.
  kriging.model <- get_kriging_model(fit.draws, small.forest.errors)
  
  # 3. To determine the optimal parameter values, predict using the kriging model at a large
  # number of random values, then select those that produced the lowest error.
  optimize.draws <- get_fit_draws(tune.num.draws, tune.parameters)
  model.surface <- predict(kriging.model, newdata = data.frame(optimize.draws), type = "SK")$mean
  tuned.params <- get_draw_parameters(optimize.draws, tune.parameters)
  
  grid <- cbind(error = c(model.surface), tuned.params)
  small.forest.optimal.draw <- which.min(grid[, "error"])
  print(paste0(type, " min idx:", small.forest.optimal.draw))
  
  # To avoid the possibility of selection bias, re-train a moderately-sized forest
  # at the value chosen by the method above
  retrained.forest.params <- grid[small.forest.optimal.draw, -1]
  for (name in default_names) {
    if (name=="alpha") {
      retrained.forest.params <- append(retrained.forest.params, 0.05)
      names(retrained.forest.params)[length(retrained.forest.params)] <- "alpha"
    } else if (name=="imbalance.penalty") {
      retrained.forest.params <- append(retrained.forest.params, 0)
      names(retrained.forest.params)[length(retrained.forest.params)] <- "imbalance.penalty"
    } else if (name=="honesty.fraction") {
      retrained.forest.params <- append(retrained.forest.params, 0.5)
      names(retrained.forest.params)[length(retrained.forest.params)] <- "honesty.fraction"
    } else if (name=="honesty.prune.leaves") {
      retrained.forest.params <- append(retrained.forest.params, TRUE)
      names(retrained.forest.params)[length(retrained.forest.params)] <- "honesty.prune.leaves"
    }
  }
  retrained.forest.params <- retrained.forest.params[names_supported] # re-order
  retrained.forest.params <- append(retrained.forest.params, moderate.num.trees)
  retrained.forest.params <- append(retrained.forest.params, type)
  retrained.forest.error <- get_forest_debiased_error(retrained.forest.params)
  
  # 4. Train a forest with default parameters, and check its predicted error.
  s_default <- scale*ceiling(nrow(X)^lower_bound_default)/nrow(X)
  forest_default <- regression_forest(X, Y, mtry=1, num.trees=num.trees, sample.fraction=s_default)
  default.forest.error <- mean(forest_default$debiased.error)
  
  # After tuning, run forests with tuned/default parameters
  if (default.forest.error < retrained.forest.error) {
    print(paste0(type, " default is used"))
    return(forest_default)
  } else {
    a <- as.double(retrained.forest.params[1])
    imb <- as.double(retrained.forest.params[2])
    h <- as.double(retrained.forest.params[3])
    prune <- as.double(retrained.forest.params[4])
    lower_bound <- (1+ (1/d)*log(1-a) / log(a))^(-1)
    s <- scale*ceiling(nrow(X)^lower_bound)/nrow(X)
    
    forest <- regression_forest(X, Y, mtry=1, num.trees=num.trees, sample.fraction=s, honesty.fraction = h, alpha = a, imbalance.penalty = imb, honesty.prune.leaves = prune)
    return(forest)
  }
}
