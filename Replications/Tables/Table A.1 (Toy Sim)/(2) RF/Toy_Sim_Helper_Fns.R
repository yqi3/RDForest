######################################
# Helper functions for toy simulations
######################################
library(dplyr)
library(DiceKriging)

#### get_fit_draws is a function that generates a dataframe with num.params columns of runif values ####
### Argument: n_draws, which is the number of random combinations
get_fit_draws <- function(n_draws){
  unif <- runif(n_draws * num.params)
  fit_draws <- matrix(unif, n_draws, num.params,
                      dimnames = list(NULL, tune.parameters))
  return(fit_draws)
}

#### get_draw_parameters is a function that produces a dataframe with random draws of parameter values ####
### Arguments: fit_draws generated by get_fit_draws
get_draw_parameters <- function(fit_draws) {
  draw_parameters <- fit_draws
  draw_parameters[,1] <- draw_parameters[,1] / 5 # random alphas
  draw_parameters[,2] <- -log(draw_parameters[,2])  # random imbalance.penalties
  draw_parameters[,3] <- 0.5 + (0.8 - 0.5) * draw_parameters[,3]  # random honesty.fractions
  draw_parameters[,4] <- ifelse(draw_parameters[,4] < 0.5, TRUE, FALSE)

  return(draw_parameters)
}

#### get_forest_debiased_error is a function that builds a forest using a particular combination of parameter values, and outputs that forest's mean squared debiased error ####
### Arguments: a row in the dataframe generated by draw_parameters, plus the number of trees and the type of the forest ("trt" or "ctrl")
get_forest_debiased_error <- function(param) {
  a <- as.double(param[1])
  imb <- as.double(param[2])
  h <- as.double(param[3])
  prune <- as.double(param[4])
  B <- as.numeric(param[5])
  type <- param[6]

  d <- ncol(feature_trt)  # dimension, same for trt and ctrl
  lower_bound <- (1+ (1/d)*log(1-a) / log(a))^(-1)

  if (type=="trt") {
    s <- scale*ceiling(nrow(trt)^lower_bound)/nrow(trt)
    forest <- regression_forest(feature_trt, trt[,1], mtry=1, num.trees=B, sample.fraction=s, honesty.fraction = h, alpha = a,
                                imbalance.penalty = imb, honesty.prune.leaves = prune)

  } else {
    s <- scale*ceiling(nrow(ctrl)^lower_bound)/nrow(ctrl)
    forest <- regression_forest(feature_ctrl, ctrl[,1], mtry=1, num.trees=B, sample.fraction=s, honesty.fraction = h, alpha = a,
                                imbalance.penalty = imb, honesty.prune.leaves = prune)
  }

  return(mean(forest$debiased.error, na.rm = TRUE))
}


#### get_kriging_model is a function that produces the Dice Kriging model for tuning ####
get_kriging_model <- function(fit_draws, small_forest_errors) {
  variance_guess <- rep(var(small_forest_errors) / 2, nrow(fit_draws))
  kriging_model <- tryCatch({
    capture.output(
      model <- km(
        design = data.frame(fit_draws),
        response = small_forest_errors,
        noise.var = variance_guess
      )
    )
    return(model)
  },
  error = function(e) {
    warning(paste0("Dicekriging threw the following error during forest tuning: \n", e))
    return(NULL)
  })
}

#### ------ GBRT is a function that gives an autotuned GBRT ------ ####
GBRT <- function(df){
  # speed up computation
  all_cores <- parallel::detectCores(logical = FALSE)
  doParallel::registerDoParallel(cores = all_cores)
  
  # Model to be tuned
  xgb_model <- boost_tree(
    mode = "regression",
    trees = tune(),
    tree_depth = tune(), min_n = tune(), 
    loss_reduction = tune(),      ## first three: model complexity
    sample_size = tune(), mtry = ncol(df)-1,         ## randomness
    learn_rate = tune(),                         ## step size
  ) %>% 
    set_engine("xgboost", objective = "reg:squarederror")
  
  # Grid of parameters to search
  xgb_grid <- grid_latin_hypercube(
    trees(),
    tree_depth(),
    loss_reduction(),
    min_n(),
    sample_size = sample_prop(),
    learn_rate(),
    size = 30
  )
  
  # Formula for the model
  xgb_wf <- workflow() %>%
    add_formula(Y ~ .) %>%
    add_model(xgb_model)
  
  # Splitting for cross validation
  myfolds <- vfold_cv(df, v=5) # 5-fold
  
  # Tunning
  xgb_res <- tune_grid(
    xgb_wf,
    resamples = myfolds,
    grid = xgb_grid,
    control = control_grid(save_pred = TRUE)
  )
  
  best_auc <- select_best(xgb_res, "rmse") # Best tuned parameters
  final_xgb <- finalize_workflow(xgb_wf, best_auc)
  
  return(final_xgb)
}
